# -*- coding: utf-8 -*-
"""Qwen_eval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10WjH0fO8q5jLOFqzBipzUAW2GD41QLp6
"""

from google.colab import drive
drive.mount('/content/drive')
import json
!pip install -U datasets
!pip install scikit-learn

from sklearn.metrics import f1_score, jaccard_score
from sklearn.preprocessing import MultiLabelBinarizer


gold_repo = f'/content/drive/MyDrive/575J/polyguard_test_1-2607.json'
output_repo = f'/content/drive/MyDrive/575J/lora_output_redo_1-2607_onwards.json'

with open(gold_repo, 'r') as g:
  gold_data = json.load(g)
with open(output_repo, 'r') as f:
  predicted_data = json.load(f)

gold_data = gold_data[:1675]
print(len(gold_data))
print(len(predicted_data))



def evaluate(all_responses, true_responses):
  '''
                      true_yes    true_no
  predicted_yes    TP [0][0]          FP [0][1]
  predicted_no     FN  [1][0]         TN [1][1]
  '''
  g_true_harmful_requests = []
  g_predicted_harmful_requests = []

  g_true_harmful_responses = []
  g_predicted_harmful_responses = []

  g_true_refusal = []
  g_predicted_refusal = []

  g_true_prompt_violations = []
  g_predicted_prompt_violations = []

  g_true_response_violations = []
  g_predicted_response_violations = []


  for (predicted_response, true_response) in zip(all_responses, true_responses):
      true_harmful_requests = []
      predicted_harmful_requests = []

      true_harmful_responses = []
      predicted_harmful_responses = []

      true_refusal = []
      predicted_refusal = []

      true_prompt_violations = []
      predicted_prompt_violations = []

      true_response_violations = []
      predicted_response_violations = []

      # For harmful_request
      if predicted_response['request_label'] == 'harmful':
        predicted_harmful_requests.append('yes')
      else:
        predicted_harmful_requests.append('no')
      if true_response['prompt_harm_label'] == 'harmful':
        true_harmful_requests.append('yes')
      else:
        true_harmful_requests.append('no')


      # For harmful_response
      if predicted_response['response_label'] == 'harmful':
        predicted_harmful_responses.append('yes')
      else:
        predicted_harmful_responses.append('no')

      if true_response['response_harm_label'] == 'harmful':
        true_harmful_responses.append('yes')
      else:
        true_harmful_responses.append('no')

      #refusal label
      if predicted_response['refusal_response'] == 'refusal':
        predicted_refusal.append('yes')
      else:
        predicted_refusal.append('no')
      if true_response['response_refusal_label'] == 'refusal':
        true_refusal.append('yes')
      else:
        true_refusal.append('no')

      #### Jaccard similarity ####
      #prompt violations

      if predicted_response['request_violations']:
        predicted_prompt_violations = predicted_response['request_violations']
      else:
        predicted_prompt_violations.append('None')

      if true_response['prompt_categories']!='':
        list_prompt_violations = true_response['prompt_categories'].split(" ")
        true_prompt_violations = list_prompt_violations
      else:
        true_prompt_violations.append('None')


      if predicted_response['response_violations']:
        predicted_response_violations = predicted_response['request_violations']
      else:
        predicted_prompt_violations.append('None')

      if true_response['response_categories']!='':
        list_response_violations = true_response['prompt_categories'].split(" ")
        true_response_violations = list_response_violations
      else:
        true_prompt_violations.append('None')

      g_true_harmful_requests.append(true_harmful_requests)
      g_predicted_harmful_requests.append(predicted_harmful_requests)
      g_true_harmful_responses.append(true_harmful_responses)
      g_predicted_harmful_responses.append(predicted_harmful_responses)
      g_true_refusal.append(true_refusal)
      g_predicted_refusal.append(predicted_refusal)
      g_true_prompt_violations.append(true_prompt_violations)
      g_predicted_prompt_violations.append(predicted_prompt_violations)
      g_true_response_violations.append(true_response_violations)
      g_predicted_response_violations.append(predicted_response_violations)
  # Calculate F1 scores for harmful_request
  f1_harmful_request = f1_score(g_true_harmful_requests, g_predicted_harmful_requests, pos_label='yes', average = 'macro')
  f1_harmful_response = f1_score(g_true_harmful_responses, g_predicted_harmful_responses, pos_label='yes', average = 'macro')
  f1_refusal = f1_score(g_true_refusal, g_predicted_refusal, pos_label='yes', average = 'macro')
  jaccard_prompt = jaccard_score(g_true_prompt_violations, g_predicted_prompt_violations, average = 'macro')
  jaccard_response = jaccard_score(g_true_response_violations, g_predicted_response_violations, average = 'macro')

  return f1_harmful_request, f1_harmful_response, f1_refusal, jaccard_prompt, jaccard_response

if __name__ == '__main__':
  f1_harmful_request, f1_harmful_response, f1_refusal, jaccard_prompt, jaccard_response = evaluate(predicted_data, gold_data)
  print(f'F1 Score for harmful_request: {f1_harmful_request}')
  print(f'F1 Score for harmful_response: {f1_harmful_response}')
  print(f'F1 Score for refusal: {f1_refusal}')
  print(f'Jaccard Score for prompt violations: {jaccard_prompt}')
  print(f'Jaccard Score for response violations: {jaccard_response}')

# %%
def evaluate(all_responses, true_responses):
  '''
                      true_yes    true_no
  predicted_yes    TP [0][0]          FP [0][1]
  predicted_no     FN  [1][0]         TN [1][1]
  '''
  g_true_harmful_requests = []
  g_predicted_harmful_requests = []

  g_true_harmful_responses = []
  g_predicted_harmful_responses = []

  g_true_refusal = []
  g_predicted_refusal = []

  g_true_prompt_violations_list = [] # Use a different name to avoid confusion
  g_predicted_prompt_violations_list = [] # Use a different name

  g_true_response_violations_list = [] # Use a different name
  g_predicted_response_violations_list = [] # Use a different name


  for (predicted_response, true_response) in zip(all_responses, true_responses):
      # For harmful_request
      if predicted_response['request_label'] == 'harmful':
        g_predicted_harmful_requests.append('yes')
      else:
        g_predicted_harmful_requests.append('no')
      if true_response['prompt_harm_label'] == 'harmful':
        g_true_harmful_requests.append('yes')
      else:
        g_true_harmful_requests.append('no')


      # For harmful_response
      if predicted_response['response_label'] == 'harmful':
        g_predicted_harmful_responses.append('yes')
      else:
        g_predicted_harmful_responses.append('no')

      if true_response['response_harm_label'] == 'harmful':
        g_true_harmful_responses.append('yes')
      else:
        g_true_harmful_responses.append('no')

      #refusal label
      if predicted_response['refusal_response'] == 'refusal':
        g_predicted_refusal.append('yes')
      else:
        g_predicted_refusal.append('no')
      if true_response['response_refusal_label'] == 'refusal':
        g_true_refusal.append('yes')
      else:
        g_true_refusal.append('no')

      #### Jaccard similarity ####
      #prompt violations

      # Collect prompt violations as lists of strings
      if predicted_response['request_violations']:
        g_predicted_prompt_violations_list.append(predicted_response['request_violations'])
      else:
        g_predicted_prompt_violations_list.append([]) # Append empty list if no violations

      if true_response['prompt_categories']!='':
        list_prompt_violations = true_response['prompt_categories'].split(" ")
        g_true_prompt_violations_list.append(list_prompt_violations)
      else:
        g_true_prompt_violations_list.append([]) # Append empty list if no violations


      # Collect response violations as lists of strings
      if predicted_response['response_violations']:
        # This was a bug in the original code, it was appending request_violations again
        g_predicted_response_violations_list.append(predicted_response['response_violations'])
      else:
        g_predicted_response_violations_list.append([]) # Append empty list if no violations

      if true_response['response_categories']!='':
        list_response_violations = true_response['response_categories'].split(" ") # Corrected to use response_categories
        g_true_response_violations_list.append(list_response_violations)
      else:
        g_true_response_violations_list.append([]) # Append empty list if no violations


  # Calculate F1 scores for harmful_request
  f1_harmful_request = f1_score(g_true_harmful_requests, g_predicted_harmful_requests, pos_label='yes', average = 'macro')
  f1_harmful_response = f1_score(g_true_harmful_responses, g_predicted_harmful_responses, pos_label='yes', average = 'macro')
  f1_refusal = f1_score(g_true_refusal, g_predicted_refusal, pos_label='yes', average = 'macro')

  # Use MultiLabelBinarizer to transform the lists of lists into binary matrices
  mlb_prompt = MultiLabelBinarizer()
  # Fit on both true and predicted to ensure all possible labels are considered
  mlb_prompt.fit(g_true_prompt_violations_list + g_predicted_prompt_violations_list)
  g_true_prompt_violations_binarized = mlb_prompt.transform(g_true_prompt_violations_list)
  g_predicted_prompt_violations_binarized = mlb_prompt.transform(g_predicted_prompt_violations_list)

  mlb_response = MultiLabelBinarizer()
  # Fit on both true and predicted to ensure all possible labels are considered
  mlb_response.fit(g_true_response_violations_list + g_predicted_response_violations_list)
  g_true_response_violations_binarized = mlb_response.transform(g_true_response_violations_list)
  g_predicted_response_violations_binarized = mlb_response.transform(g_predicted_response_violations_list)


  # Calculate Jaccard scores using the binarized data
  jaccard_prompt = jaccard_score(g_true_prompt_violations_binarized, g_predicted_prompt_violations_binarized, average = 'macro')
  jaccard_response = jaccard_score(g_true_response_violations_binarized, g_predicted_response_violations_binarized, average = 'macro')

  return f1_harmful_request, f1_harmful_response, f1_refusal, jaccard_prompt, jaccard_response

def evaluate(all_responses, true_responses):
  '''
                      true_yes    true_no
  predicted_yes    TP [0][0]          FP [0][1]
  predicted_no     FN  [1][0]         TN [1][1]
  '''
  g_true_harmful_requests = []
  g_predicted_harmful_requests = []

  g_true_harmful_responses = []
  g_predicted_harmful_responses = []

  g_true_refusal = []
  g_predicted_refusal = []

  g_true_prompt_violations = []
  g_predicted_prompt_violations = []

  g_true_response_violations = []
  g_predicted_response_violations = []

  avg_prompt_violations = []
  avg_response_violations = []

  for (predicted_response, true_response) in zip(all_responses, true_responses):
      true_harmful_requests = []
      predicted_harmful_requests = []

      true_harmful_responses = []
      predicted_harmful_responses = []

      true_refusal = []
      predicted_refusal = []

      true_prompt_violations = []
      predicted_prompt_violations = []

      true_response_violations = []
      predicted_response_violations = []

      # For harmful_request
      if predicted_response['request_label'] == 'harmful':
        predicted_harmful_requests.append('yes')
      else:
        predicted_harmful_requests.append('no')
      if true_response['prompt_harm_label'] == 'harmful':
        true_harmful_requests.append('yes')
      else:
        true_harmful_requests.append('no')


      # For harmful_response
      if predicted_response['response_label'] == 'harmful':
        predicted_harmful_responses.append('yes')
      else:
        predicted_harmful_responses.append('no')

      if true_response['response_harm_label'] == 'harmful':
        true_harmful_responses.append('yes')
      else:
        true_harmful_responses.append('no')

      #refusal label
      if predicted_response['refusal_response'] == 'refusal':
        predicted_refusal.append('yes')
      else:
        predicted_refusal.append('no')
      if true_response['response_refusal_label'] == 'refusal':
        true_refusal.append('yes')
      else:
        true_refusal.append('no')

      #### Jaccard similarity ####
      #prompt violations

      if predicted_response['request_violations']:
        print('sanity', predicted_response['request_violations'])
        predicted_prompt_violations = predicted_response['request_violations']
      elif not predicted_response['request_violations']:
        predicted_prompt_violations.append('None')

      if true_response['prompt_categories']!='':
        list_prompt_violations = true_response['prompt_categories'].split(" ")
        true_prompt_violations = list_prompt_violations
      else:
        true_prompt_violations.append('None')


      if predicted_response['response_violations']:
        #print('sanity', predicted_response['response_violations'])
        predicted_response_violations = predicted_response['request_violations']
      else:
        predicted_response_violations.append('None')

      if true_response['response_categories']!='':
        list_response_violations = true_response['prompt_categories'].split(" ")
        true_response_violations = list_response_violations
      else:
        true_response_violations.append('None')

      g_true_harmful_requests.append(true_harmful_requests)
      g_predicted_harmful_requests.append(predicted_harmful_requests)
      g_true_harmful_responses.append(true_harmful_responses)
      g_predicted_harmful_responses.append(predicted_harmful_responses)
      g_true_refusal.append(true_refusal)
      g_predicted_refusal.append(predicted_refusal)
      g_true_prompt_violations.append(true_prompt_violations)
      g_predicted_prompt_violations.append(predicted_prompt_violations)
      g_true_response_violations.append(true_response_violations)
      g_predicted_response_violations.append(predicted_response_violations)

      print(f'true_prompt_violations: {true_prompt_violations}')
      print(f'predicted_prompt_violations: {predicted_prompt_violations}')
      print(f'true_response_violations: {true_response_violations}')
      print(f'predicted_response_violations: {predicted_response_violations}')
      jaccard_request = jaccard_compute(true_prompt_violations, predicted_prompt_violations)
      avg_prompt_violations.append(jaccard_request)
      jaccard_response = jaccard_compute(true_response_violations, predicted_response_violations)
      avg_response_violations.append(jaccard_response)

  # Calculate F1 scores for harmful_request
  f1_harmful_request = f1_score(g_true_harmful_requests, g_predicted_harmful_requests, pos_label='yes', average = 'macro')
  f1_harmful_response = f1_score(g_true_harmful_responses, g_predicted_harmful_responses, pos_label='yes', average = 'macro')
  f1_refusal = f1_score(g_true_refusal, g_predicted_refusal, pos_label='yes', average = 'macro')
  #jaccard_prompt = jaccard_score(g_true_prompt_violations, g_predicted_prompt_violations, average = 'macro')
  #jaccard_response = jaccard_score(g_true_response_violations, g_predicted_response_violations, average = 'macro')

  return f1_harmful_request, f1_harmful_response, f1_refusal, avg_prompt_violations, avg_response_violations

def jaccard_compute(true_violations, predicted_violations):
  #base case:
  if not true_violations or not predicted_violations:
    return 0.0
  if not true_violations and not predicted_violations:
    return 1.0


  true_set = set(true_violations)
  true_set_ = remove_none(true_set)
  predicted_set = set(predicted_violations)
  predicted_set_ = remove_none(predicted_set)
  print('true_set_', true_set_)
  print('predicted_set_', predicted_set_)
  intersection = true_set.intersection(predicted_set_)
  union = true_set.union(predicted_set_)
  jaccard_score = len(intersection) / len(union)
  return jaccard_score

def remove_none(violations):
  violations_copy = violations.copy()

  if violations_copy and 'None' in violations_copy and len(violations_copy - {'None'}) > 0:
    violations.remove('None')
  return violations

from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.metrics import f1_score, jaccard_score

def evaluate(all_responses, true_responses):
    g_true_harmful_requests = []
    g_predicted_harmful_requests = []

    g_true_harmful_responses = []
    g_predicted_harmful_responses = []

    g_true_refusal = []
    g_predicted_refusal = []

    g_true_prompt_violations = []
    g_predicted_prompt_violations = []

    g_true_response_violations = []
    g_predicted_response_violations = []

    for predicted_response, true_response in zip(all_responses, true_responses):
        # For harmful_request
        predicted_harmful_requests = 'yes' if predicted_response['request_label'] == 'harmful' else 'no'
        true_harmful_requests = 'yes' if true_response['prompt_harm_label'] == 'harmful' else 'no'

        # For harmful_response
        predicted_harmful_responses = 'yes' if predicted_response['response_label'] == 'harmful' else 'no'
        true_harmful_responses = 'yes' if true_response['response_harm_label'] == 'harmful' else 'no'

        # refusal label
        predicted_refusal = 'yes' if predicted_response['refusal_response'] == 'refusal' else 'no'
        true_refusal = 'yes' if true_response['response_refusal_label'] == 'refusal' else 'no'

        # prompt violations
        predicted_prompt_violations = predicted_response['request_violations'] if predicted_response['request_violations'] else ['None']
        true_prompt_violations = true_response['prompt_categories'].split(" ") if true_response['prompt_categories'] else ['None']

        # response violations
        predicted_response_violations = predicted_response['response_violations'] if predicted_response['response_violations'] else ['None']
        true_response_violations = true_response['response_categories'].split(" ") if true_response['response_categories'] else ['None']

        g_true_harmful_requests.append(true_harmful_requests)
        g_predicted_harmful_requests.append(predicted_harmful_requests)
        g_true_harmful_responses.append(true_harmful_responses)
        g_predicted_harmful_responses.append(predicted_harmful_responses)
        g_true_refusal.append(true_refusal)
        g_predicted_refusal.append(predicted_refusal)
        g_true_prompt_violations.append(true_prompt_violations)
        g_predicted_prompt_violations.append(predicted_prompt_violations)
        g_true_response_violations.append(true_response_violations)
        g_predicted_response_violations.append(predicted_response_violations)

    # Convert multi-label data to binary format
    mlb = MultiLabelBinarizer()

    # Fit on all possible categories (combine true and predicted to get all classes)
    all_categories = set()
    for violations in g_true_prompt_violations + g_predicted_prompt_violations + g_true_response_violations + g_predicted_response_violations:
        all_categories.update(violations)
    mlb.fit([all_categories])

    # Transform the true and predicted violations
    true_prompt_bin = mlb.transform(g_true_prompt_violations)
    pred_prompt_bin = mlb.transform(g_predicted_prompt_violations)
    true_response_bin = mlb.transform(g_true_response_violations)
    pred_response_bin = mlb.transform(g_predicted_response_violations)

    # Calculate F1 scores for binary classifications
    f1_harmful_request = f1_score(g_true_harmful_requests, g_predicted_harmful_requests, pos_label='yes', average='macro')
    f1_harmful_response = f1_score(g_true_harmful_responses, g_predicted_harmful_responses, pos_label='yes', average='macro')
    f1_refusal = f1_score(g_true_refusal, g_predicted_refusal, pos_label='yes', average='macro')

    # Calculate Jaccard scores for multi-label classifications
    jaccard_prompt = jaccard_score(true_prompt_bin, pred_prompt_bin, average='macro')
    jaccard_response = jaccard_score(true_response_bin, pred_response_bin, average='macro')

    return f1_harmful_request, f1_harmful_response, f1_refusal, jaccard_prompt, jaccard_response

#### combine data for adalora

with open('/content/drive/MyDrive/575J/adalora_outputs/adalora_1-389.json','r') as f:
  data_1 = json.load(f)

with open('/content/drive/MyDrive/575J/adalora_outputs/adalora_output_390-600_onwards.json','r') as f:
  data_2 = json.load(f)

with open('/content/drive/MyDrive/575J/adalora_outputs/adalora_output_601-900_onwards.json','r') as f:
  data_3 = json.load(f)

with open('/content/drive/MyDrive/575J/adalora_outputs/adalora_output_901-1200_onwards.json','r') as f:
  data_4 = json.load(f)

with open('/content/drive/MyDrive/575J/adalora_outputs/adalora_output_1201-1500_onwards.json','r') as f:
  data_5 = json.load(f)
with open('/content/drive/MyDrive/575J/adalora_outputs/adalora_output_1801-2100_onwards.json','r') as f:
  data_6 = json.load(f)
with open('/content/drive/MyDrive/575J/adalora_outputs/adalora_output_2101-2400_onwards.json','r') as f:
  data_7= json.load(f)
with open('/content/drive/MyDrive/575J/adalora_outputs/adalora_output_2401-2557_onwards.json','r') as f:
  data_8 = json.load(f)

predicted_data = data_1 + data_2 + data_3 + data_4 + data_5 + data_6 + data_7 + data_8
print(len(predicted_data))

new_gold_1 = gold_data[:1501]
print((len(new_gold_1)))
new_gold_2 = gold_data[1801:2557]
print(len(new_gold_2))
gold_data = new_gold_1 + new_gold_2
print(len(gold_data))

if __name__ == '__main__':
  f1_harmful_request, f1_harmful_response, f1_refusal, avg_prompt_violations, avg_response_violations = evaluate(predicted_data, gold_data)
  import numpy as np
  print(f'F1 Score for harmful_request: {f1_harmful_request}')
  print(f'F1 Score for harmful_response: {f1_harmful_response}')
  print(f'F1 Score for refusal: {f1_refusal}')
  print(avg_prompt_violations)
  print(f'Jaccard Score for prompt violations: {np.mean(avg_prompt_violations)}')
  print(f'Jaccard Score for response violations: {np.mean(avg_response_violations)}')