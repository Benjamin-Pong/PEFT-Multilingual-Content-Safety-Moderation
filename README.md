This project is the first comparative study of al-
ternative parameter-efficient finetuning meth-
ods to Low Rank Adaptation (LoRA), notably
Adaptive Low-Rank Adaptation (AdaLoRA)
and Weight-Decomposition Low-Rank Adap-
tation (DoRA), for downstream multilingual
content moderation tasks which is a multi-step
classification task. Preliminary experimental
results on the PolyGuard datasets (Kumar et al.,
2025) demonstrate the potential for these al-
ternative parameter-efficient fine-tuning meth-
ods in outperforming LoRA and full-finetuning
for a subset of content moderation tasks. This
study also reveals the challenges of decoder-
only models for classification tasks by incor-
porating In-Context-Learning and Constraint
Decoding techniques. More rigorous experi-
ments need to be done to further support the
claim that AdaLORA and DORA can both sur-
pass LoRA and full-finetuning for cross-lingual
content moderation tasks.
